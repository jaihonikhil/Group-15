# SemEvalTask9
Tables are ubiquitous in documents and presentations for conveying important information in a concise manner. This is true in many domains, stretching from scientific to government documents.  In fact,  surrounding text in these articles are often statements summarizing or highlighting some information derived from the primary source data in tables. Describing all the information provided in a table in a readable manner would be lengthy and considerably more difficult to understand. We present a task for statement verification and evidence finding using tables from scientific articles. This important task promotes proper interpretation of the surrounding article.  
Link to the Google Doc for Evaluation: https://docs.google.com/document/d/18vKVh2e3HbjhCvunKiYBTxqOTPFdh4vLBGKgarnHG9A/edit?ts=5f68409e

Link to the Competition: https://sites.google.com/view/sem-tab-facts

# BreakingBERT@IITK at SemEval-2021

Task 9 : Statement Verification and Evidence Finding with Tables


Recently, there has been an interest in factual
verification and prediction over structured data
like tables and graphs. To circumvent any false
news incident, it is necessary to not only model
and predict over structured data efficiently but
also to explain those predictions. In this paper, as part of the SemEval-2021 Task 9, we
tackle the problem of fact verification and evidence finding over tabular data. There are two
subtasks. Given a table and a statement/fact,
subtask A determines whether the statement is
inferred from the tabular data, and subtask B
determines which cells in the table provide evidence for the former subtask. We make a comparison of the baselines and state-of-the-art approaches over the given SemTabFact dataset.
We also propose a novel approach CellBERT
to solve evidence finding as a form of the Natural Language Inference task. We obtain a 3-
way F1 score of 0.69 on subtask A and an F1
score of 0.65 on subtask B.

Link to the arxiv paper: https://arxiv.org/pdf/2104.03071.pdf

